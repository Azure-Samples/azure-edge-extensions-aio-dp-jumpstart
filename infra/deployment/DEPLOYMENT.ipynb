{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Azure IO Data Processor Pipelines with yaml\n",
    "\n",
    "This README provides guidance on deploying the Azure IO Data Processor Dataset and Pipelines using the provided yaml files.\n",
    "\n",
    "## Resources created\n",
    "\n",
    "After following this README the next resources would be created:\n",
    "\n",
    "1. HTTP Endpoint to Load the Reference Data\n",
    "1. `dataset-reference-data` Dataset to store reference data\n",
    "1. Pipeline to get the reference data from the http server and put it in the `dataset-reference-data` Dataset\n",
    "1. Machine Status Calculation Pipeline\n",
    "1. `dataset-shift-history-totals` dataset to store totals in shifts\n",
    "1. Pipeline to calculate current shift total\n",
    "1. Pipeline to calculate totals in shifts and load data to `dataset-shift-history-totals` Dataset\n",
    "1. Pipeline to calculate total count\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before deploying the Azure IO Data Processor Dataset and Pipeline, ensure you have the following prerequisites:\n",
    "\n",
    "1. You must have an active Azure subscription. \n",
    "   - If you don't have an Azure subscription, you can create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.\n",
    "1. An Azure Resource Group where all your resources are deployed\n",
    "1. An Azure Arc enabled Kubernetes cluster\n",
    "1. An Azure Data Processor instance\n",
    "1. An AIO Message Queue (MQ) instance\n",
    "\n",
    "For steps on manually deploying these, you can find instructions [here](../provisioning/PROVISIONING.ipynb).\n",
    "\n",
    "> **Note:** For more information on JQ queries used in the pipelines, check out the [JQ Notebook](../../docs/JQ.ipynb)\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Run the following script to deploy http server, datasets, pipeline to load the reference data, machine status calculation pipeline and total counter calculation pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/azure-edge-extensions-aio-dp-jumpstart/infra/deployment/http-server-ref /workspaces/azure-edge-extensions-aio-dp-jumpstart/infra/deployment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (1/2)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (2/2)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 179B                                       0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 179B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-slim         0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (2/4)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 179B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-slim         0.2s\n",
      " => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (3/4)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 179B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-slim         0.4s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (3/4)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 179B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-slim         0.5s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.8s (3/4)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 179B                                       0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-slim         0.7s\n",
      "\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (4/4)                                          docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 179B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9-slim         0.8s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.1s (10/10)                                        docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 179B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9-slim         0.8s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/library/python:3.9-slim@sha256:e0bc011bb55918109  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 85B                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/4] COPY reference-data-server.py .                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/4] COPY reference-data.json .                                0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:e58f877dd9459d1ddb33d8f8ae3126b4f7f759b36efe0  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/ref-http-server                         0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (10/10) FINISHED                               docker:default\n",
      "\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 179B                                       0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9-slim         0.8s\n",
      "\u001b[0m\u001b[34m => [auth] library/python:pull token for registry-1.docker.io              0.0s\n",
      "\u001b[0m\u001b[34m => [1/4] FROM docker.io/library/python:3.9-slim@sha256:e0bc011bb55918109  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 85B                                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [2/4] WORKDIR /app                                              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/4] COPY reference-data-server.py .                           0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/4] COPY reference-data.json .                                0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:e58f877dd9459d1ddb33d8f8ae3126b4f7f759b36efe0  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/ref-http-server                         0.0s\n",
      "\u001b[0m\u001b[?25hUsing default tag: latest\n",
      "The push refers to repository [localhost:5500/ref-http-server]\n",
      "\n",
      "\u001b[1B408d780a: Preparing \n",
      "\u001b[1Be4d2ea08: Preparing \n",
      "\u001b[1Bb5756502: Preparing \n",
      "\u001b[1B521e7cb2: Preparing \n",
      "\u001b[1B503a99a8: Preparing \n",
      "\u001b[1B49bf7f54: Preparing \n",
      "\u001b[1B3bfdf54e: Preparing \n",
      "\u001b[1Blatest: digest: sha256:2167779974e38b78efe7854cebd6e94ab34fe1b1b39b7889e9c358fdd093c6bb size: 1990\n",
      "/workspaces/azure-edge-extensions-aio-dp-jumpstart/infra/deployment\n",
      "Docker image has been built and pushed to the local k3d registry.\n",
      "dataset.dataprocessor.iotoperations.azure.com/dataset-reference-data created\n",
      "dataset.dataprocessor.iotoperations.azure.com/dataset-shift-history-totals created\n",
      "NAME                           AGE\n",
      "dataset-shift-history-totals   0s\n",
      "dataset-reference-data         0s\n",
      "dataset.dataprocessor.iotoperations.azure.com/dataset-reference-data condition met\n",
      "dataset.dataprocessor.iotoperations.azure.com/dataset-shift-history-totals condition met\n",
      "Datasets have successfully been provisioned.\n",
      "service/ref-data-service created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-current-shift-counts created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-http-reference-data-load created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-machine-status created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-metrics-to-grafana created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-preprocess-opcua-payload created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-refdata-list created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-reference-data-load created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-shift-history-list created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-shift-history-totals-load created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-shift-history-totals-load-manual created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-total-counter created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-zurich-simulator-machine-status created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-zurich-validation created\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-zurich-validation-debug created\n",
      "pod/ref-data-server created\n",
      "NAME                                        AGE\n",
      "pipeline-current-shift-counts               0s\n",
      "pipeline-reference-data-load                0s\n",
      "pipeline-shift-history-list                 0s\n",
      "pipeline-http-reference-data-load           0s\n",
      "pipeline-shift-history-totals-load          0s\n",
      "pipeline-shift-history-totals-load-manual   0s\n",
      "pipeline-machine-status                     0s\n",
      "pipeline-total-counter                      0s\n",
      "pipeline-zurich-simulator-machine-status    0s\n",
      "pipeline-zurich-validation                  0s\n",
      "pipeline-preprocess-opcua-payload           0s\n",
      "pipeline-zurich-validation-debug            0s\n",
      "pipeline-metrics-to-grafana                 0s\n",
      "pipeline-refdata-list                       0s\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-current-shift-counts condition met\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-reference-data-load condition met\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-shift-history-list condition met\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-shift-history-totals-load condition met\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-total-counter condition met\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-zurich-validation condition met\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-zurich-validation-debug condition met\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-http-reference-data-load condition met\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-machine-status condition met\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-metrics-to-grafana condition met\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-refdata-list condition met\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-zurich-simulator-machine-status condition met\n",
      "pipeline.dataprocessor.iotoperations.azure.com/pipeline-preprocess-opcua-payload condition met\n",
      "Pipelines have successfully been provisioned.\n"
     ]
    }
   ],
   "source": [
    "./01-aio-deploy-dp-pipelines.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Open MQTTUI in a new terminal to watch the data flow through the pipelines:\n",
    "\n",
    "   ```bash\n",
    "      mqttui\n",
    "   ```\n",
    "\n",
    "3. Publish an input message to the `zurich/input/valid` topic to calculate a machine status, open a new terminal and execute the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# publish an input to the zurich/input/valid topic to trigger the machine calculation pipeline\n",
    "mosquitto_pub -t \"zurich/input/valid\" -f \"../../test/data/machine-status-samples/Idle-true.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "   You should now see the enriched message with the reference data in the `metrics/aio/machine-status` topic. The machine status value should be `Idle` for the current input.\n",
    "   To output a different machine status value, you can update the input message accordingly by following the [machine status design](../../docs/design/machine-status.md#machine-status-logic).\n",
    "\n",
    "   ![Image of MQTTUI with metrics/aio/machine-status topic](../../docs/assets/machine-status-aio.png)\n",
    "\n",
    "3. Publish an input message to the `zurich/input/valid` topic to calculate a total counter, open a new terminal and execute the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# publish an input to the zurich/input/valid topic to trigger the machine calculation pipeline\n",
    "mosquitto_pub -t \"zurich/input/valid\" -f \"../../test/data/total-counter-samples/Good-Counter-5.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   You should now see the enriched message with the reference data in the `metrics/aio/total-count` topic. The total counter value should be 5 for the current input.\n",
    "   To output a different total counter value, you can send more input message accordingly by following the [total counter design](../../docs/design/total-count.md#total-count-logic).\n",
    "\n",
    "   ![Image of MQTTUI with metrics/aio/total-count topic](../../docs/assets/total-count-aio.png)\n",
    "\n",
    "## Metrics pipeline\n",
    "\n",
    "The metrics pipeline creates a payload that follows the [OpenTelemetry Protocol (OTLP) Specification](https://github.com/open-telemetry/opentelemetry-proto) to send metrics to an [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/).\n",
    "\n",
    "This collector is part of the AIO base installation, and you can verify if it is running with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/aio-otel-collector-df45659b6-d9hr9             1/1     Running   0          3m15s\n",
      "service/aio-otel-collector              ClusterIP      10.43.106.217   <none>        8889/TCP,4317/TCP,4318/TCP                                                                                  3m15s\n",
      "deployment.apps/aio-otel-collector            1/1     1            1           3m15s\n",
      "replicaset.apps/aio-otel-collector-df45659b6             1         1         1       3m15s\n"
     ]
    }
   ],
   "source": [
    "kubectl get all -n azure-iot-operations | grep otel-collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The `aio-otel-collector` and `aio-akri-otel-collector` are different services, we use the `aio-otel-collector` by default.\n",
    "\n",
    "By default this OTEL collector will send the metrics and traces to a prometheus collector configured in the [Observability guide](../../docs/OBSERVABILITY.md).\n",
    "\n",
    "The pipeline can listen to multiple topics (as `metrics/aio/machine-status` and `metrics/aio/total-count`), and generate an HTTP Request to the OTEL collector as shown in the following diagram:\n",
    "\n",
    "```mermaid\n",
    "C4Context\n",
    "   title Metrics\n",
    "   \n",
    "   Boundary(b0, \"AIO\", \"Kubernetes cluster\") {\n",
    "      Boundary(dp, \"Data Processing\") {\n",
    "         System(P1, \"Metrics pipeline\")\n",
    "      }\n",
    "\n",
    "      Component(HTTP, \"OTEL Collector\", \"HTTP server\")\n",
    "\n",
    "      Boundary(b1, \"MQTT Broker\") {\n",
    "         ComponentQueue(MQTT, \"MQTT Broker\", \"\")\n",
    "      }\n",
    "   }\n",
    "\n",
    "   Component(RD1, \"Azure Managed Grafana\", \"Grafana service\")\n",
    "   \n",
    "   Rel(HTTP, RD1, \"Send metrics/traces\", \"Prometheus format\")\n",
    "   \n",
    "   Rel(P1, HTTP, \"Send metrics\", \"POST /v1/metrics\")\n",
    "   BiRel(P1, MQTT, \"Pub/Sub\")\n",
    "   \n",
    "   UpdateRelStyle(HTTP, RD1, $textColor=\"gray\", $lineColor=\"gray\", $offsetY=\"-50\", $offsetX=\"-50\")\n",
    "   UpdateRelStyle(P1, HTTP, $textColor=\"black\", $lineColor=\"gray\", $offsetY=\"-70\", $offsetX=\"-50\")\n",
    "   UpdateRelStyle(P1, MQTT, $textColor=\"black\", $lineColor=\"gray\", $offsetY=\"-20\", $offsetX=\"-23\")\n",
    "   \n",
    "   UpdateLayoutConfig($c4ShapeInRow=\"1\", $c4BoundaryInRow=\"3\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detailed view of the pipeline looks like this:\n",
    "\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "   A[metrics/input] -->|\"{ PlantId: Contoso, \\n LineId: Campus-A, \\n MachineId: 3459, \\n VariableId: MACHINE_STATUS, \\n Value: Idle }\"| B(HTTP Payload generator stage)\n",
    "   B -->|Open Telemetry Metrics JSON payload | C(HTTP POST Request to /v1/metrics)\n",
    "   C -->|original payload + request + response| E[fa:fa-car metrics/debug]\n",
    "```\n",
    "\n",
    "At the end metrics/debug will contain the original payload, the request and the response for troubleshooting purposes.\n",
    "\n",
    "### OPCUA Messages Simulator\n",
    "\n",
    "You can provision the simulator into the system with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./08-simulator.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a pod (data-simulator) that will constantly run and send messages every 5 seconds with the two payloads of Good and MachineStatus.\n",
    "\n",
    "You can open MQTTUI to visualize all the messages flowing in this order:\n",
    "\n",
    "1. opcua-simulator\n",
    "1. opcua\n",
    "1. input\n",
    "1. input/valid or input/invalid\n",
    "1. current-shift-total\n",
    "1. metrics/aio/machine-status and metrics/aio/total-count\n",
    "\n",
    "![MQTTUI All Topics](../../docs/assets/mqttui-alltopics.png)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Run e2e Integration tests on the deployed pipelines in [test/integration](../../test/integration/README.md).\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "To clean up the deployed resources, use the following script to delete Azure resources and recreate a fresh k3d cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "../provisioning/00-clean-up.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "In an effort to maintain consistency across our project, we adhere to the following naming conventions:\n",
    "\n",
    "- **Pipeline CRDs:** Both the filename and the pipeline metadata name will start with \"pipeline-\".\n",
    "- **Dataset CRDs:** Both the filename and the dataset metadata name will start with \"dataset-\".\n",
    "\n",
    "By following these conventions, we ensure a uniform structure and make it easier to manage and locate resources within the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
